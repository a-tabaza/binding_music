{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_no_contraction import Encoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder.load_from_checkpoint(\"/workspace/fairouz/logs/vggish_randne_openclip_mxbai/version_0/checkpoints/epoch=199-step=40000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (audio_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (text_encoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (graph_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (combined_encoder): Sequential(\n",
       "    (0): Linear(in_features=1792, out_features=3584, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3584, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "import json\n",
    "tracks = json.load(open('fairouz_conf/fairouz/tracks_contextualized.json'))\n",
    "track_ids = list(tracks.keys())\n",
    "\n",
    "audio_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/audio/song_audio_vggish_embeddings.json'))\n",
    "graph_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/graph/karate_club/song_nodes_RandNE_embedding.json'))\n",
    "image_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/image/album_covers_openclip_embeddings.json'))\n",
    "text_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/lyrics/song_lyrics_mxbai_embeddings.json'))\n",
    "\n",
    "audio_embeddings_dict = transform_dict(audio_embeddings)\n",
    "graph_embeddings_dict = transform_dict(graph_embeddings)\n",
    "image_embeddings_dict = transform_dict(image_embeddings)\n",
    "text_embeddings_dict = transform_dict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = json.load(open('/workspace/fairouz/fairouz_conf/fairouz/positives_negatives.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for track_id in track_ids:\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(image).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(text).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(graph).unsqueeze(0).to(\"cuda\"),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    embeddings.append({\"id\": track_id, \"embedding\": embedding.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = []\n",
    "metatadata_array = []\n",
    "for emb in embeddings:\n",
    "    id = emb['id']\n",
    "    emb = np.array(emb['embedding'])\n",
    "    metadata = tracks[id]\n",
    "    md = {\n",
    "        \"id\": id,\n",
    "        \"track_title\": metadata['track_title'],\n",
    "        \"artist_name\": metadata['artist_name'],\n",
    "        \"album_name\": metadata['album_name'],\n",
    "        \"context\": \", \".join(metadata[\"lyrics\"]['context']),\n",
    "        \"summary\": metadata[\"lyrics\"]['summary'],\n",
    "        \"emotional\": \", \".join(metadata[\"lyrics\"]['emotional']),\n",
    "        \"genre\": metadata['genres'][0] if len(metadata['genres']) > 0 else 'None',\n",
    "        \"image\": metadata['image'],\n",
    "        \"preview_url\": metadata['preview_url'],\n",
    "    }\n",
    "    embeddings_array.append(emb)\n",
    "    metatadata_array.append(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(np.array(embeddings_array).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairouz_embedding(track_id):\n",
    "    audio, graph, image, text = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positives(track_id):\n",
    "    return load[track_id][\"positives\"]\n",
    "\n",
    "def get_negatives(track_id):\n",
    "    return load[track_id][\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.retrieval import retrieval_precision\n",
    "from torchmetrics.functional.retrieval import retrieval_recall\n",
    "from torchmetrics.functional.retrieval import retrieval_hit_rate\n",
    "from torchmetrics.functional.retrieval import retrieval_average_precision\n",
    "from torchmetrics.functional.retrieval import retrieval_reciprocal_rank\n",
    "from torchmetrics.functional.retrieval import retrieval_normalized_dcg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"retrieval_precision\": retrieval_precision, # top-k\n",
    "    \"retrieval_recall\": retrieval_recall, # top-k\n",
    "    \"retrieval_hit_rate\": retrieval_hit_rate, # top-k\n",
    "    \"retrieval_average_precision\": retrieval_average_precision, # top-k\n",
    "    \"retrieval_reciprocal_rank\": retrieval_reciprocal_rank, # top-k\n",
    "    \"retrieval_normalized_dcg\": retrieval_normalized_dcg, # top-k\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(track_id, k=10):\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(image).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(text).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(graph).unsqueeze(0).to(\"cuda\"),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True)\n",
    "    D, I = index.search(embedding, k)\n",
    "    distances = D[0]\n",
    "    normalized_distances = (distances - np.min(distances)) / (\n",
    "        np.max(distances) - np.min(distances)\n",
    "    )\n",
    "    m = nn.Softmax(dim=0)\n",
    "    similarity = m(torch.tensor([1 - d for d in normalized_distances]))\n",
    "    positives = get_positives(track_id)\n",
    "    ids = [metatadata_array[i][\"id\"] for i in I[0]]\n",
    "    target = [1 if id in positives else 0 for id in ids]\n",
    "    return similarity, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval_precision_k@10 0.07445256\n",
      "retrieval_precision_k@15 0.06326034\n",
      "retrieval_precision_k@20 0.055778593\n",
      "retrieval_precision_k@25 0.050170314\n",
      "retrieval_recall_k@10 0.5231144\n",
      "retrieval_recall_k@15 0.62895375\n",
      "retrieval_recall_k@20 0.69343066\n",
      "retrieval_recall_k@25 0.7457421\n",
      "retrieval_hit_rate_k@10 0.5231144\n",
      "retrieval_hit_rate_k@15 0.62895375\n",
      "retrieval_hit_rate_k@20 0.69343066\n",
      "retrieval_hit_rate_k@25 0.7457421\n",
      "retrieval_average_precision_k@10 0.14876954\n",
      "retrieval_average_precision_k@15 0.15344614\n",
      "retrieval_average_precision_k@20 0.15399164\n",
      "retrieval_average_precision_k@25 0.15365708\n",
      "retrieval_reciprocal_rank_k@10 0.14886263\n",
      "retrieval_reciprocal_rank_k@15 0.15715034\n",
      "retrieval_reciprocal_rank_k@20 0.16080293\n",
      "retrieval_reciprocal_rank_k@25 0.16311963\n",
      "retrieval_normalized_dcg_k@10 0.24258032\n",
      "retrieval_normalized_dcg_k@15 0.2707632\n",
      "retrieval_normalized_dcg_k@20 0.28666085\n",
      "retrieval_normalized_dcg_k@25 0.29858273\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    k = {10: [], 15: [], 20: [], 25: [],}\n",
    "    for track_id in track_ids:\n",
    "        sim, target = retrieve(track_id, 10)\n",
    "        k[10].append(metrics[metric](sim, target, 10))\n",
    "        sim, target = retrieve(track_id, 15)\n",
    "        k[15].append(metrics[metric](sim, target, 15))\n",
    "        sim, target = retrieve(track_id, 20)\n",
    "        k[20].append(metrics[metric](sim, target, 20))\n",
    "        sim, target = retrieve(track_id, 25)\n",
    "        k[25].append(metrics[metric](sim, target, 25))\n",
    "    print(f\"{metric}_k@10\", np.mean(k[10]))\n",
    "    print(f\"{metric}_k@15\", np.mean(k[15]))\n",
    "    print(f\"{metric}_k@20\", np.mean(k[20]))\n",
    "    print(f\"{metric}_k@25\", np.mean(k[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())\n",
    "# print as (X.X M)\n",
    "print(f\"{sum(p.numel() for p in model.parameters()) / 1e6:.1f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
