{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import lightning as L\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dict(list_of_dict):\n",
    "    transformed_embedding = {}\n",
    "    for dictionary in list_of_dict:\n",
    "        transformed_embedding[dictionary['id']] = dictionary['embedding']\n",
    "\n",
    "    return transformed_embedding\n",
    "\n",
    "def get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict):\n",
    "    audio_embedding = audio_embeddings_dict[track_id]\n",
    "    graph_embedding = graph_embeddings_dict[track_id]\n",
    "    image_embedding = image_embeddings_dict[track_id]\n",
    "    text_embedding = text_embeddings_dict[track_id]\n",
    "    return audio_embedding, graph_embedding, image_embedding, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "\n",
    "tracks = json.load(open('fairouz_conf/fairouz/tracks_contextualized.json'))\n",
    "track_ids = list(tracks.keys())\n",
    "\n",
    "audio_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/audio/song_audio_vggish_embeddings.json'))\n",
    "graph_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/graph/karate_club/song_nodes_RandNE_embedding.json'))\n",
    "image_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/image/album_covers_openclip_embeddings.json'))\n",
    "text_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/lyrics/song_lyrics_mxbai_embeddings.json'))\n",
    "\n",
    "audio_embeddings_dict = transform_dict(audio_embeddings)\n",
    "graph_embeddings_dict = transform_dict(graph_embeddings)\n",
    "image_embeddings_dict = transform_dict(image_embeddings)\n",
    "text_embeddings_dict = transform_dict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Tensors\n",
    "load = json.load(open('fairouz_conf/fairouz/positives_negatives.json'))\n",
    "\n",
    "def generate_data():\n",
    "    anchor_audio = []\n",
    "    anchor_graph = []\n",
    "    anchor_image = []\n",
    "    anchor_text  = []\n",
    "\n",
    "    query_audio = []\n",
    "    query_graph = []\n",
    "    query_image = []\n",
    "    query_text = []\n",
    "    \n",
    "    label = []\n",
    "\n",
    "    for track_id, data in load.items():\n",
    "        anchor = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "        for positive in data['positives']:\n",
    "            audio_embd, graph_embd, img_embd, text_embd = get_modality_embeddings(positive, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "\n",
    "            anchor_audio.append(anchor[0])\n",
    "            anchor_graph.append(anchor[1])\n",
    "            anchor_image.append(anchor[2])\n",
    "            anchor_text.append(anchor[3])\n",
    "\n",
    "            query_audio.append(audio_embd)\n",
    "            query_graph.append(graph_embd)\n",
    "            query_image.append(img_embd)\n",
    "            query_text.append(text_embd)\n",
    "\n",
    "            label.append(1)\n",
    "        \n",
    "        for negative in data['negatives']:\n",
    "            audio_embd, graph_embd, img_embd, text_embd = get_modality_embeddings(negative, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "\n",
    "            anchor_audio.append(anchor[0])\n",
    "            anchor_graph.append(anchor[1])\n",
    "            anchor_image.append(anchor[2])\n",
    "            anchor_text.append(anchor[3])\n",
    "\n",
    "            query_audio.append(audio_embd)\n",
    "            query_graph.append(graph_embd)\n",
    "            query_image.append(img_embd)\n",
    "            query_text.append(text_embd)\n",
    "\n",
    "            label.append(0)\n",
    "        \n",
    "    return torch.tensor(anchor_audio), torch.tensor(anchor_graph), torch.tensor(anchor_image), torch.tensor(anchor_text), \\\n",
    "           torch.tensor(query_audio), torch.tensor(query_graph), torch.tensor(query_image), torch.tensor(query_text), \\\n",
    "           torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_audio, anchor_graph, anchor_image, anchor_text, query_audio, query_graph, query_image, query_text, label = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6375, 128]),\n",
       " torch.Size([6375, 128]),\n",
       " torch.Size([6375, 512]),\n",
       " torch.Size([6375, 1024]),\n",
       " torch.Size([6375, 128]),\n",
       " torch.Size([6375, 128]),\n",
       " torch.Size([6375, 512]),\n",
       " torch.Size([6375, 1024]),\n",
       " torch.Size([6375]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_audio.shape, anchor_graph.shape, anchor_image.shape, anchor_text.shape, query_audio.shape, query_graph.shape, query_image.shape, query_text.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_size,\n",
    "        image_size,\n",
    "        text_size,\n",
    "        graph_size,\n",
    "        expansion_factor,\n",
    "        contraction_factor,\n",
    "        embedding_size,\n",
    "        drop_out=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.audio_encoder = nn.Sequential(\n",
    "            nn.Linear(audio_size, audio_size // contraction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "        self.image_encoder = nn.Sequential(\n",
    "            nn.Linear(image_size, image_size // contraction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "        self.text_encoder = nn.Sequential(\n",
    "            nn.Linear(text_size, text_size // contraction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "        self.graph_encoder = nn.Sequential(\n",
    "            nn.Linear(graph_size, graph_size // contraction_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "        self.cat_size = (\n",
    "            audio_size // contraction_factor\n",
    "            + image_size // contraction_factor\n",
    "            + text_size // contraction_factor\n",
    "            + graph_size // contraction_factor\n",
    "        )\n",
    "        self.combined_encoder = nn.Sequential(\n",
    "            nn.Linear(self.cat_size, self.cat_size * expansion_factor),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(self.cat_size * expansion_factor, embedding_size),\n",
    "            nn.Dropout(drop_out),\n",
    "        )\n",
    "        self.distance_metric = lambda x, y: F.pairwise_distance(x, y, p=2)\n",
    "        self.margin = 0.5\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, anchor, query, labels):\n",
    "        anchor_audio, anchor_image, anchor_text, anchor_graph = anchor\n",
    "        query_audio, query_image, query_text, query_graph = query\n",
    "        anchor_embedding = self.combined_encoder(\n",
    "            torch.cat(\n",
    "                (\n",
    "                    self.audio_encoder(anchor_audio),\n",
    "                    self.image_encoder(anchor_image),\n",
    "                    self.text_encoder(anchor_text),\n",
    "                    self.graph_encoder(anchor_graph),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        )\n",
    "        query_embedding = self.combined_encoder(\n",
    "            torch.cat(\n",
    "                (\n",
    "                    self.audio_encoder(query_audio),\n",
    "                    self.image_encoder(query_image),\n",
    "                    self.text_encoder(query_text),\n",
    "                    self.graph_encoder(query_graph),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        )\n",
    "        return anchor_embedding, query_embedding, labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        anchor, query, labels = batch\n",
    "        anchor_embedding, query_embedding, labels = self(anchor, query, labels)\n",
    "        distances = self.distance_metric(anchor_embedding, query_embedding)\n",
    "        losses = 0.5 * labels[0].float() * distances.pow(2) + (\n",
    "            1 - labels[0]\n",
    "        ).float() * F.relu(self.margin - distances).pow(2)\n",
    "        loss = losses.mean()\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, audio, image, text, graph):\n",
    "        audio = self.audio_encoder(audio)\n",
    "        image = self.image_encoder(image)\n",
    "        text = self.text_encoder(text)\n",
    "        graph = self.graph_encoder(graph)\n",
    "        embedding = self.combined_encoder(torch.cat((audio, image, text, graph), dim=1))\n",
    "        return embedding\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of your inputs\n",
    "audio_size = 128\n",
    "graph_size = 128\n",
    "image_size = 512\n",
    "text_size = 1024\n",
    "batch_size = 32\n",
    "emb_size = 128\n",
    "expansion_factor = 2\n",
    "contraction_factor = 4\n",
    "drop_out = 0.2\n",
    "\n",
    "model = Encoder(audio_size, image_size, text_size, graph_size, expansion_factor, contraction_factor, emb_size, drop_out)\n",
    "\n",
    "# Generate random data for each input type\n",
    "anchor_audio_data = anchor_audio\n",
    "anchor_image_data = anchor_image\n",
    "anchor_text_data = anchor_text\n",
    "anchor_graph_data = anchor_graph\n",
    "query_audio_data = query_audio\n",
    "query_image_data = query_image\n",
    "query_text_data = query_text\n",
    "query_graph_data = query_graph\n",
    "labels_data = label\n",
    "# Assuming labels are binary\n",
    "\n",
    "# Combine the data into a single dataset\n",
    "anchors = data.TensorDataset(anchor_audio_data, anchor_image_data, anchor_text_data, anchor_graph_data)\n",
    "queries = data.TensorDataset(query_audio_data, query_image_data, query_text_data, query_graph_data)\n",
    "labels = data.TensorDataset(labels_data)\n",
    "dataset = data.StackDataset(anchors, queries, labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "data_loader = data.DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (audio_encoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (image_encoder): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (text_encoder): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (graph_encoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (combined_encoder): Sequential(\n",
      "    (0): Linear(in_features=448, out_features=896, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=896, out_features=128, bias=True)\n",
      "    (4): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import CSVLogger\n",
    "logger = CSVLogger(\"logs\", name=\"vggish_randne_openclip_mxbai_contraction_4_expansion2_dropout_uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(min_epochs=100, max_epochs=200, logger = logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/vggish_randne_openclip_mxbai_contraction_4_expansion2_dropout_uniform\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | audio_encoder    | Sequential | 4.1 K \n",
      "1 | image_encoder    | Sequential | 65.7 K\n",
      "2 | text_encoder     | Sequential | 262 K \n",
      "3 | graph_encoder    | Sequential | 4.1 K \n",
      "4 | combined_encoder | Sequential | 517 K \n",
      "------------------------------------------------\n",
      "853 K     Trainable params\n",
      "0         Non-trainable params\n",
      "853 K     Total params\n",
      "3.414     Total estimated model params size (MB)\n",
      "/workspace/fairouz/training_venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 200/200 [00:03<00:00, 62.01it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199: 100%|██████████| 200/200 [00:03<00:00, 60.07it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio, image, text, graph (model)\n",
    "# audio_embedding, graph_embedding, image_embedding, text_embedding (get modality embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (audio_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (text_encoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (graph_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (combined_encoder): Sequential(\n",
       "    (0): Linear(in_features=448, out_features=896, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=896, out_features=128, bias=True)\n",
       "    (4): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for track_id in track_ids:\n",
    "    audio, graph, image, text = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    embeddings.append({'id': track_id, 'embedding': embedding.tolist()})\n",
    "\n",
    "json.dump(embeddings, open('fairouz_conf/fairouz/embeddings/combined/embeddings_contracted_dropout_all_200_epoch.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = []\n",
    "metatadata_array = []\n",
    "for emb in embeddings:\n",
    "    id = emb['id']\n",
    "    emb = np.array(emb['embedding'])\n",
    "    metadata = tracks[id]\n",
    "    md = {\n",
    "        \"id\": id,\n",
    "        \"track_title\": metadata['track_title'],\n",
    "        \"artist_name\": metadata['artist_name'],\n",
    "        \"album_name\": metadata['album_name'],\n",
    "        \"context\": \", \".join(metadata[\"lyrics\"]['context']),\n",
    "        \"summary\": metadata[\"lyrics\"]['summary'],\n",
    "        \"emotional\": \", \".join(metadata[\"lyrics\"]['emotional']),\n",
    "        \"genre\": metadata['genres'][0] if len(metadata['genres']) > 0 else 'None',\n",
    "        \"image\": metadata['image'],\n",
    "        \"preview_url\": metadata['preview_url'],\n",
    "    }\n",
    "    embeddings_array.append(emb)\n",
    "    metatadata_array.append(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-25 20:44:32.709\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m96\u001b[0m - \u001b[33m\u001b[1mAn ID field was not specified in your data so one was generated for you in insertion order.\u001b[0m\n",
      "\u001b[32m2024-03-25 20:44:35.817\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_create_project\u001b[0m:\u001b[36m868\u001b[0m - \u001b[1mCreating dataset `fairouz-vggish-randne-openclip-mxbai-200-epochs-contracted-dropout-all-euclidian`\u001b[0m\n",
      "\u001b[32m2024-03-25 20:44:36.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mUploading data to Atlas.\u001b[0m\n",
      "1it [00:01,  1.08s/it]\n",
      "\u001b[32m2024-03-25 20:44:37.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36m_add_data\u001b[0m:\u001b[36m1537\u001b[0m - \u001b[1mUpload succeeded.\u001b[0m\n",
      "\u001b[32m2024-03-25 20:44:37.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.atlas\u001b[0m:\u001b[36mmap_data\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1m`tyqnology/fairouz-vggish-randne-openclip-mxbai-200-epochs-contracted-dropout-all-euclidian`: Data upload succeeded to dataset`\u001b[0m\n",
      "\u001b[32m2024-03-25 20:44:37.583\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1116\u001b[0m - \u001b[33m\u001b[1mYou did not specify the `topic_label_field` option in your topic_model, your dataset will not contain auto-labeled topics.\u001b[0m\n",
      "\u001b[32m2024-03-25 20:44:38.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.dataset\u001b[0m:\u001b[36mcreate_index\u001b[0m:\u001b[36m1246\u001b[0m - \u001b[1mCreated map `fairouz_vggish_randne_openclip_mxbai_200_epochs_contracted_dropout_all_euclidian` in dataset `tyqnology/fairouz-vggish-randne-openclip-mxbai-200-epochs-contracted-dropout-all-euclidian`: https://atlas.nomic.ai/data/tyqnology/fairouz-vggish-randne-openclip-mxbai-200-epochs-contracted-dropout-all-euclidian/map\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nomic import atlas\n",
    "\n",
    "dataset = atlas.map_data(\n",
    "    data=metatadata_array,\n",
    "    embeddings=np.array(embeddings_array),\n",
    "    identifier=\"fairouz_vggish_randne_openclip_mxbai_200_epochs_contracted_dropout_all_euclidian\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(np.array(embeddings_array).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairouz_embedding(track_id):\n",
    "    audio, graph, image, text = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positives(track_id):\n",
    "    return load[track_id][\"positives\"]\n",
    "\n",
    "def get_negatives(track_id):\n",
    "    return load[track_id][\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional.ranking import retrieval_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(track_id, k=10):\n",
    "    audio, graph, image, text = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True)\n",
    "    D, I = index.search(embedding, k)\n",
    "    distances = D[0]\n",
    "    normalized_distances = (distances - np.min(distances)) / (np.max(distances) - np.min(distances))\n",
    "    m = nn.Softmax(dim=0)\n",
    "    similarity = m(torch.tensor([1-d for d in normalized_distances]))\n",
    "    positives = get_positives(track_id)\n",
    "    ids = [metatadata_array[i][\"id\"] for i in I[0]]\n",
    "    target = [1 if id in positives else 0 for id in ids]\n",
    "    return similarity, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 0.07177616\n",
      "Precision@15 0.05993512\n",
      "Precision@20 0.052858885\n",
      "Precision@25 0.048175182\n"
     ]
    }
   ],
   "source": [
    "p_at_10 = []\n",
    "p_at_15 = []\n",
    "p_at_20 = []\n",
    "p_at_25 = []\n",
    "for track_id in track_ids:\n",
    "    sim, target = evaluate(track_id, 10)\n",
    "    p_at_10.append(retrieval_precision(sim, target, 10))\n",
    "    sim, target = evaluate(track_id, 15)\n",
    "    p_at_15.append(retrieval_precision(sim, target, 15))\n",
    "    sim, target = evaluate(track_id, 20)\n",
    "    p_at_20.append(retrieval_precision(sim, target, 20))\n",
    "    sim, target = evaluate(track_id, 25)\n",
    "    p_at_25.append(retrieval_precision(sim, target, 25))\n",
    "\n",
    "print(\"Precision@10\", np.mean(p_at_10))\n",
    "print(\"Precision@15\", np.mean(p_at_15))\n",
    "print(\"Precision@20\", np.mean(p_at_20))\n",
    "print(\"Precision@25\", np.mean(p_at_25))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'track_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m p_at_4 \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m track_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrack_ids\u001b[49m:\n\u001b[1;32m      3\u001b[0m     sim, target \u001b[38;5;241m=\u001b[39m evaluate(track_id, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      4\u001b[0m     p_at_4\u001b[38;5;241m.\u001b[39mappend(retrieval_precision(sim, target, \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'track_ids' is not defined"
     ]
    }
   ],
   "source": [
    "p_at_4 = []\n",
    "for track_id in track_ids:\n",
    "    sim, target = evaluate(track_id, 4)\n",
    "    p_at_4.append(retrieval_precision(sim, target, 4))\n",
    "\n",
    "print(\"Precision@4\", np.mean(p_at_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_between(track_id1, track_id2):\n",
    "    return F.pairwise_distance(torch.tensor(get_fairouz_embedding(track_id1)).unsqueeze(0), torch.tensor(get_fairouz_embedding(track_id2)).unsqueeze(0), p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
