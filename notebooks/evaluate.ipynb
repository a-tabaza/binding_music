{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.retrieval import retrieval_hit_rate\n",
    "from torchmetrics.functional.retrieval import retrieval_precision\n",
    "from torchmetrics.functional.retrieval import retrieval_average_precision\n",
    "from torchmetrics.functional.retrieval import retrieval_reciprocal_rank\n",
    "from torchmetrics.functional.retrieval import retrieval_normalized_dcg\n",
    "from torchmetrics.functional.retrieval import retrieval_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Encoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder.load_from_checkpoint(\n",
    "    \"../vggish_randne_openclip_mxbai_contraction_2_expansion4_51k_datapoints/version_0/checkpoints/epoch=199-step=322800.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (audio_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (image_encoder): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (text_encoder): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (graph_encoder): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (combined_encoder): Sequential(\n",
       "    (0): Linear(in_features=896, out_features=3584, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3584, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "import json\n",
    "tracks = json.load(open('../fairouz_conf/fairouz/tracks_contextualized.json'))\n",
    "track_ids = list(tracks.keys())\n",
    "\n",
    "audio_embeddings = json.load(open(\n",
    "    '../fairouz_conf/fairouz/embeddings/audio/song_audio_vggish_embeddings.json'))\n",
    "graph_embeddings = json.load(open(\n",
    "    '../fairouz_conf/fairouz/embeddings/graph/karate_club/song_nodes_RandNE_embedding.json'))\n",
    "image_embeddings = json.load(open(\n",
    "    '../fairouz_conf/fairouz/embeddings/image/album_covers_openclip_embeddings.json'))\n",
    "text_embeddings = json.load(open(\n",
    "    '../fairouz_conf/fairouz/embeddings/lyrics/song_lyrics_mxbai_embeddings.json'))\n",
    "\n",
    "audio_embeddings_dict = transform_dict(audio_embeddings)\n",
    "graph_embeddings_dict = transform_dict(graph_embeddings)\n",
    "image_embeddings_dict = transform_dict(image_embeddings)\n",
    "text_embeddings_dict = transform_dict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = json.load(open(\"../fairouz_conf/fairouz/positives_negatives_more_negatives.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for track_id in track_ids:\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(image).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(text).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(graph).unsqueeze(0).to(DEVICE),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    embeddings.append({\"id\": track_id, \"embedding\": embedding.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = []\n",
    "metatadata_array = []\n",
    "for emb in embeddings:\n",
    "    id = emb[\"id\"]\n",
    "    emb = np.array(emb[\"embedding\"])\n",
    "    metadata = tracks[id]\n",
    "    md = {\n",
    "        \"id\": id,\n",
    "        \"track_title\": metadata[\"track_title\"],\n",
    "        \"artist_name\": metadata[\"artist_name\"],\n",
    "        \"album_name\": metadata[\"album_name\"],\n",
    "        \"context\": \", \".join(metadata[\"lyrics\"][\"context\"]),\n",
    "        \"summary\": metadata[\"lyrics\"][\"summary\"],\n",
    "        \"emotional\": \", \".join(metadata[\"lyrics\"][\"emotional\"]),\n",
    "        \"genre\": metadata[\"genres\"][0] if len(metadata[\"genres\"]) > 0 else \"None\",\n",
    "        \"image\": metadata[\"image\"],\n",
    "        \"preview_url\": metadata[\"preview_url\"],\n",
    "    }\n",
    "    embeddings_array.append(emb)\n",
    "    metatadata_array.append(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(np.array(embeddings_array).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairouz_embedding(track_id):\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(\n",
    "        image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positives(track_id):\n",
    "    return load[track_id][\"positives\"]\n",
    "\n",
    "\n",
    "def get_negatives(track_id):\n",
    "    return load[track_id][\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"retrieval_precision\": retrieval_precision,\n",
    "    \"retrieval_recall\": retrieval_recall,\n",
    "    \"retrieval_hit_rate\": retrieval_hit_rate,\n",
    "    \"retrieval_average_precision\": retrieval_average_precision,\n",
    "    \"retrieval_reciprocal_rank\": retrieval_reciprocal_rank,\n",
    "    \"retrieval_normalized_dcg\": retrieval_normalized_dcg,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(track_id, k=10):\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(image).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(text).unsqueeze(0).to(DEVICE),\n",
    "        torch.tensor(graph).unsqueeze(0).to(DEVICE),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True)\n",
    "    D, I = index.search(embedding, k)\n",
    "    distances = D[0]\n",
    "    normalized_distances = (distances - np.min(distances)) / (\n",
    "        np.max(distances) - np.min(distances)\n",
    "    )\n",
    "    m = nn.Softmax(dim=0)\n",
    "    similarity = m(torch.tensor([1 - d for d in normalized_distances]))\n",
    "    positives = get_positives(track_id)\n",
    "    ids = [metatadata_array[i][\"id\"] for i in I[0]]\n",
    "    target = [1 if id in positives else 0 for id in ids]\n",
    "    return similarity, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval_precision_k@10 0.3281022\n",
      "retrieval_precision_k@15 0.29878345\n",
      "retrieval_precision_k@20 0.27670315\n",
      "retrieval_precision_k@25 0.2596107\n",
      "retrieval_recall_k@10 0.31484777\n",
      "retrieval_recall_k@15 0.42805424\n",
      "retrieval_recall_k@20 0.5288602\n",
      "retrieval_recall_k@25 0.62133265\n",
      "retrieval_hit_rate_k@10 0.9306569\n",
      "retrieval_hit_rate_k@15 0.96836984\n",
      "retrieval_hit_rate_k@20 0.9829684\n",
      "retrieval_hit_rate_k@25 0.99026763\n",
      "retrieval_average_precision_k@10 0.41307124\n",
      "retrieval_average_precision_k@15 0.4041722\n",
      "retrieval_average_precision_k@20 0.3920268\n",
      "retrieval_average_precision_k@25 0.38168234\n",
      "retrieval_reciprocal_rank_k@10 0.361479\n",
      "retrieval_reciprocal_rank_k@15 0.36450228\n",
      "retrieval_reciprocal_rank_k@20 0.3652504\n",
      "retrieval_reciprocal_rank_k@25 0.3655893\n",
      "retrieval_normalized_dcg_k@10 0.32183042\n",
      "retrieval_normalized_dcg_k@15 0.35890967\n",
      "retrieval_normalized_dcg_k@20 0.40776482\n",
      "retrieval_normalized_dcg_k@25 0.4517281\n"
     ]
    }
   ],
   "source": [
    "for metric in metrics:\n",
    "    for k in [10, 15, 20, 25]:\n",
    "        results = {f\"{metric}_k@{k}\": []}\n",
    "        for track_id in track_ids:\n",
    "            similarity, target = evaluate(track_id, 50)\n",
    "            results[f\"{metric}_k@{k}\"].append(metrics[metric]\n",
    "                                              (similarity, target, k))\n",
    "        print(f\"{metric}_k@{k}\", np.mean(results[f\"{metric}_k@{k}\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
