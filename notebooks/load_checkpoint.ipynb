{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import lightning as L\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_dropout import Encoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Encoder.load_from_checkpoint(\"/workspace/fairouz/logs/vggish_randne_openclip_mxbai_contraction_4_expansion2_dropout_modality_encoders/version_0/checkpoints/epoch=199-step=40000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data\n",
    "import json\n",
    "tracks = json.load(open('fairouz_conf/fairouz/tracks_contextualized.json'))\n",
    "track_ids = list(tracks.keys())\n",
    "\n",
    "audio_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/audio/song_audio_vggish_embeddings.json'))\n",
    "graph_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/graph/karate_club/song_nodes_RandNE_embedding.json'))\n",
    "image_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/image/album_covers_openclip_embeddings.json'))\n",
    "text_embeddings = json.load(open('fairouz_conf/fairouz/embeddings/lyrics/song_lyrics_mxbai_embeddings.json'))\n",
    "\n",
    "audio_embeddings_dict = transform_dict(audio_embeddings)\n",
    "graph_embeddings_dict = transform_dict(graph_embeddings)\n",
    "image_embeddings_dict = transform_dict(image_embeddings)\n",
    "text_embeddings_dict = transform_dict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = json.load(open('fairouz_conf/fairouz/positives_negatives.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for track_id in track_ids:\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(image).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(text).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(graph).unsqueeze(0).to(\"cuda\"),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    embeddings.append({\"id\": track_id, \"embedding\": embedding.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(embeddings, open('fairouz_conf/fairouz/embeddings/combined/embeddings_contracted_textimage8_graphaudio4_expansion2_200_epoch.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = []\n",
    "metatadata_array = []\n",
    "for emb in embeddings:\n",
    "    id = emb['id']\n",
    "    emb = np.array(emb['embedding'])\n",
    "    metadata = tracks[id]\n",
    "    md = {\n",
    "        \"id\": id,\n",
    "        \"track_title\": metadata['track_title'],\n",
    "        \"artist_name\": metadata['artist_name'],\n",
    "        \"album_name\": metadata['album_name'],\n",
    "        \"context\": \", \".join(metadata[\"lyrics\"]['context']),\n",
    "        \"summary\": metadata[\"lyrics\"]['summary'],\n",
    "        \"emotional\": \", \".join(metadata[\"lyrics\"]['emotional']),\n",
    "        \"genre\": metadata['genres'][0] if len(metadata['genres']) > 0 else 'None',\n",
    "        \"image\": metadata['image'],\n",
    "        \"preview_url\": metadata['preview_url'],\n",
    "    }\n",
    "    embeddings_array.append(emb)\n",
    "    metatadata_array.append(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(np.array(embeddings_array).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairouz_embedding(track_id):\n",
    "    audio, graph, image, text = get_modality_embeddings(track_id, audio_embeddings_dict, image_embeddings_dict, text_embeddings_dict, graph_embeddings_dict)\n",
    "    embedding = model.predict_step(torch.tensor(audio).unsqueeze(0), torch.tensor(image).unsqueeze(0), torch.tensor(text).unsqueeze(0), torch.tensor(graph).unsqueeze(0))\n",
    "    embedding = embedding.numpy(force=True).squeeze()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positives(track_id):\n",
    "    return load[track_id][\"positives\"]\n",
    "\n",
    "def get_negatives(track_id):\n",
    "    return load[track_id][\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional.ranking import retrieval_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(track_id, k=10):\n",
    "    audio, graph, image, text = get_modality_embeddings(\n",
    "        track_id,\n",
    "        audio_embeddings_dict,\n",
    "        image_embeddings_dict,\n",
    "        text_embeddings_dict,\n",
    "        graph_embeddings_dict,\n",
    "    )\n",
    "    embedding = model.predict_step(\n",
    "        torch.tensor(audio).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(image).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(text).unsqueeze(0).to(\"cuda\"),\n",
    "        torch.tensor(graph).unsqueeze(0).to(\"cuda\"),\n",
    "    )\n",
    "    embedding = embedding.numpy(force=True)\n",
    "    D, I = index.search(embedding, k)\n",
    "    distances = D[0]\n",
    "    normalized_distances = (distances - np.min(distances)) / (\n",
    "        np.max(distances) - np.min(distances)\n",
    "    )\n",
    "    m = nn.Softmax(dim=0)\n",
    "    similarity = m(torch.tensor([1 - d for d in normalized_distances]))\n",
    "    positives = get_positives(track_id)\n",
    "    ids = [metatadata_array[i][\"id\"] for i in I[0]]\n",
    "    target = [1 if id in positives else 0 for id in ids]\n",
    "    return similarity, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10 0.10437957\n",
      "Precision@15 0.08759124\n",
      "Precision@20 0.07585159\n",
      "Precision@25 0.06583942\n"
     ]
    }
   ],
   "source": [
    "p_at_10 = []\n",
    "p_at_15 = []\n",
    "p_at_20 = []\n",
    "p_at_25 = []\n",
    "for track_id in track_ids:\n",
    "    sim, target = evaluate(track_id, 10)\n",
    "    p_at_10.append(retrieval_precision(sim, target, 10))\n",
    "    sim, target = evaluate(track_id, 15)\n",
    "    p_at_15.append(retrieval_precision(sim, target, 15))\n",
    "    sim, target = evaluate(track_id, 20)\n",
    "    p_at_20.append(retrieval_precision(sim, target, 20))\n",
    "    sim, target = evaluate(track_id, 25)\n",
    "    p_at_25.append(retrieval_precision(sim, target, 25))\n",
    "\n",
    "print(\"Precision@10\", np.mean(p_at_10))\n",
    "print(\"Precision@15\", np.mean(p_at_15))\n",
    "print(\"Precision@20\", np.mean(p_at_20))\n",
    "print(\"Precision@25\", np.mean(p_at_25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@4 0.13746959\n"
     ]
    }
   ],
   "source": [
    "p_at_4 = []\n",
    "for track_id in track_ids:\n",
    "    sim, target = evaluate(track_id, 2)\n",
    "    p_at_4.append(retrieval_precision(sim, target, 2))\n",
    "\n",
    "print(\"Precision@4\", np.mean(p_at_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (audio_encoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (image_encoder): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (text_encoder): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (graph_encoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (combined_encoder): Sequential(\n",
      "    (0): Linear(in_features=448, out_features=896, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=896, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 M\n"
     ]
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())\n",
    "# print as (X.X M)\n",
    "print(f\"{sum(p.numel() for p in model.parameters()) / 1e6:.1f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nomic import atlas\n",
    "\n",
    "dataset = atlas.map_data(\n",
    "    data=metatadata_array,\n",
    "    embeddings=np.array(embeddings_array),\n",
    "    identifier=\"fairouz_vggish_randne_openclip_mxbai_200_epochs_contracted_dropout_euclidian\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
